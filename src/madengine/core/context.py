#!/usr/bin/env python3
"""Module of context class.

This module contains the class to determine context.

Classes:
    Context: Class to determine context.

Functions:
    update_dict: Update dictionary.

Copyright (c) Advanced Micro Devices, Inc. All rights reserved.
"""
# built-in modules
import ast
import json
import collections.abc
import os
import re
import typing

# third-party modules
from madengine.core.console import Console


def update_dict(d: typing.Dict, u: typing.Dict) -> typing.Dict:
    """Update dictionary.

    Args:
        d: The dictionary.
        u: The update dictionary.

    Returns:
        dict: The updated dictionary.
    """
    # Update a dictionary with another dictionary, recursively.
    for k, v in u.items():
        # if the value is a dictionary, recursively update it, otherwise update the value.
        if isinstance(v, collections.abc.Mapping):
            d[k] = update_dict(d.get(k, {}), v)
        else:
            d[k] = v
    return d


class Context:
    """Class to determine context.

    Attributes:
        console: The console.
        ctx: The context.
        _gpu_context_initialized: Flag to track if GPU context is initialized.
        _system_context_initialized: Flag to track if system context is initialized.
        _build_only_mode: Flag to indicate if running in build-only mode.

    Methods:
        get_ctx_test: Get context test.
        get_gpu_vendor: Get GPU vendor.
        get_host_os: Get host OS.
        get_numa_balancing: Get NUMA balancing.
        get_system_ngpus: Get system number of GPUs.
        get_system_gpu_architecture: Get system GPU architecture.
        get_docker_gpus: Get Docker GPUs.
        get_gpu_renderD_nodes: Get GPU renderD nodes.
        set_multi_node_runner: Sets multi-node runner context.
        init_system_context: Initialize system-specific context.
        init_gpu_context: Initialize GPU-specific context for runtime.
        init_build_context: Initialize build-specific context.
        init_runtime_context: Initialize runtime-specific context.
        ensure_system_context: Ensure system context is initialized.
        ensure_runtime_context: Ensure runtime context is initialized.
        filter: Filter.
    """

    def __init__(
        self,
        additional_context: str = None,
        additional_context_file: str = None,
        build_only_mode: bool = False,
    ) -> None:
        """Constructor of the Context class.

        Args:
            additional_context: The additional context.
            additional_context_file: The additional context file.
            build_only_mode: Whether running in build-only mode (no GPU detection).

        Raises:
            RuntimeError: If GPU detection fails and not in build-only mode.
        """
        # Initialize the console
        self.console = Console()
        self._gpu_context_initialized = False
        self._build_only_mode = build_only_mode
        self._system_context_initialized = False

        # Initialize base context
        self.ctx = {}

        # Initialize docker contexts as empty - will be populated based on mode
        self.ctx["docker_build_arg"] = {}
        self.ctx["docker_env_vars"] = {}

        # Read and update MAD SECRETS env variable (can be used for both build and run)
        mad_secrets = {}
        for key in os.environ:
            if "MAD_SECRETS" in key:
                mad_secrets[key] = os.environ[key]
        if mad_secrets:
            update_dict(self.ctx["docker_build_arg"], mad_secrets)
            update_dict(self.ctx["docker_env_vars"], mad_secrets)

        # Additional contexts provided in file override detected contexts
        if additional_context_file:
            with open(additional_context_file) as f:
                update_dict(self.ctx, json.load(f))

        # Additional contexts provided in command-line override detected contexts and contexts in file
        if additional_context:
            # Convert the string representation of python dictionary to a dictionary.
            dict_additional_context = ast.literal_eval(additional_context)
            update_dict(self.ctx, dict_additional_context)

        # Initialize context based on mode
        # User-provided contexts will not be overridden by detection
        if not build_only_mode:
            # For full workflow mode, initialize everything (legacy behavior preserved)
            self.init_runtime_context()
        else:
            # For build-only mode, only initialize what's needed for building
            self.init_build_context()

        ## ADD MORE CONTEXTS HERE ##

    def init_build_context(self) -> None:
        """Initialize build-specific context.

        This method sets up only the context needed for Docker builds,
        avoiding GPU detection that would fail on build-only nodes.
        System-specific contexts (host_os, numa_balancing, etc.) should be
        provided via --additional-context for build-only nodes if needed.
        """
        print("Initializing build-only context...")

        # Initialize only essential system contexts if not provided via additional_context
        if "host_os" not in self.ctx:
            try:
                self.ctx["host_os"] = self.get_host_os()
                print(f"Detected host OS: {self.ctx['host_os']}")
            except Exception as e:
                print(f"Warning: Could not detect host OS on build node: {e}")
                print(
                    "Consider providing host_os via --additional-context if needed for build"
                )

        # Don't detect GPU-specific contexts in build-only mode
        # These should be provided via additional_context if needed for build args
        if "MAD_SYSTEM_GPU_ARCHITECTURE" not in self.ctx.get("docker_build_arg", {}):
            print(
                "Info: MAD_SYSTEM_GPU_ARCHITECTURE not provided - should be set via --additional-context for GPU-specific builds"
            )

        # Handle multi-node configuration for build phase
        self._setup_build_multi_node_context()

        # Don't initialize NUMA balancing check for build-only nodes
        # This is runtime-specific and should be handled on execution nodes

    def init_runtime_context(self) -> None:
        """Initialize runtime-specific context.

        This method sets up the full context including system and GPU detection
        for nodes that will run containers.
        """
        print("Initializing runtime context with system and GPU detection...")

        # Initialize system context first
        self.init_system_context()

        # Initialize GPU context
        self.init_gpu_context()

        # Setup runtime multi-node runner
        self._setup_runtime_multi_node_context()

    def init_system_context(self) -> None:
        """Initialize system-specific context.

        This method detects system configuration like OS, NUMA balancing, etc.
        Should be called on runtime nodes to get actual execution environment context.
        """
        if self._system_context_initialized:
            return

        print("Detecting system configuration...")

        try:
            # Initialize system contexts if not already provided via additional_context
            if "ctx_test" not in self.ctx:
                self.ctx["ctx_test"] = self.get_ctx_test()

            if "host_os" not in self.ctx:
                self.ctx["host_os"] = self.get_host_os()
                print(f"Detected host OS: {self.ctx['host_os']}")

            if "numa_balancing" not in self.ctx:
                self.ctx["numa_balancing"] = self.get_numa_balancing()

                # Check if NUMA balancing is enabled or disabled.
                if self.ctx["numa_balancing"] == "1":
                    print("Warning: numa balancing is ON ...")
                elif self.ctx["numa_balancing"] == "0":
                    print("Warning: numa balancing is OFF ...")
                else:
                    print("Warning: unknown numa balancing setup ...")

            self._system_context_initialized = True

        except Exception as e:
            print(f"Warning: System context detection failed: {e}")
            if not self._build_only_mode:
                raise RuntimeError(
                    f"System context detection failed on runtime node: {e}"
                )

    def init_gpu_context(self) -> None:
        """Initialize GPU-specific context for runtime.

        This method detects GPU configuration and sets up environment variables
        needed for container execution. Should only be called on GPU nodes.
        User-provided GPU contexts will not be overridden.

        Raises:
            RuntimeError: If GPU detection fails.
        """
        if self._gpu_context_initialized:
            return

        print("Detecting GPU configuration...")

        try:
            # GPU vendor detection - only if not provided by user
            if "gpu_vendor" not in self.ctx:
                self.ctx["gpu_vendor"] = self.get_gpu_vendor()
                print(f"Detected GPU vendor: {self.ctx['gpu_vendor']}")
            else:
                print(f"Using provided GPU vendor: {self.ctx['gpu_vendor']}")

            # Initialize docker env vars for runtime - only if not already set
            if "MAD_GPU_VENDOR" not in self.ctx["docker_env_vars"]:
                self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] = self.ctx["gpu_vendor"]

            if "MAD_SYSTEM_NGPUS" not in self.ctx["docker_env_vars"]:
                self.ctx["docker_env_vars"][
                    "MAD_SYSTEM_NGPUS"
                ] = self.get_system_ngpus()

            if "MAD_SYSTEM_GPU_ARCHITECTURE" not in self.ctx["docker_env_vars"]:
                self.ctx["docker_env_vars"][
                    "MAD_SYSTEM_GPU_ARCHITECTURE"
                ] = self.get_system_gpu_architecture()

            if "MAD_SYSTEM_HIP_VERSION" not in self.ctx["docker_env_vars"]:
                self.ctx["docker_env_vars"][
                    "MAD_SYSTEM_HIP_VERSION"
                ] = self.get_system_hip_version()

            # Also add to build args (for runtime builds) - only if not already set
            if "MAD_SYSTEM_GPU_ARCHITECTURE" not in self.ctx["docker_build_arg"]:
                self.ctx["docker_build_arg"]["MAD_SYSTEM_GPU_ARCHITECTURE"] = self.ctx[
                    "docker_env_vars"
                ]["MAD_SYSTEM_GPU_ARCHITECTURE"]

            # Docker GPU configuration - only if not already set
            if "docker_gpus" not in self.ctx:
                self.ctx["docker_gpus"] = self.get_docker_gpus()

            if "gpu_renderDs" not in self.ctx:
                self.ctx["gpu_renderDs"] = self.get_gpu_renderD_nodes()

            # Default multi-node configuration - only if not already set
            if "multi_node_args" not in self.ctx:
                self.ctx["multi_node_args"] = {
                    "RUNNER": "torchrun",
                    "MAD_RUNTIME_NGPUS": self.ctx["docker_env_vars"][
                        "MAD_SYSTEM_NGPUS"
                    ],  # Use system's GPU count
                    "NNODES": 1,
                    "NODE_RANK": 0,
                    "MASTER_ADDR": "localhost",
                    "MASTER_PORT": 6006,
                    "HOST_LIST": "",
                    "NCCL_SOCKET_IFNAME": "",
                    "GLOO_SOCKET_IFNAME": "",
                }

            self._gpu_context_initialized = True

        except Exception as e:
            if self._build_only_mode:
                print(
                    f"Warning: GPU detection failed in build-only mode (expected): {e}"
                )
            else:
                raise RuntimeError(f"GPU detection failed: {e}")

    def ensure_runtime_context(self) -> None:
        """Ensure runtime context is initialized.

        This method should be called before any runtime operations
        that require system and GPU context.
        """
        if not self._system_context_initialized and not self._build_only_mode:
            self.init_system_context()
        if not self._gpu_context_initialized and not self._build_only_mode:
            self.init_gpu_context()

    def ensure_system_context(self) -> None:
        """Ensure system context is initialized.

        This method should be called when system context is needed
        but may not be initialized (e.g., in build-only mode).
        """
        if not self._system_context_initialized:
            self.init_system_context()

    def get_ctx_test(self) -> str:
        """Get context test.

        Returns:
            str: The output of the shell command.

        Raises:
            RuntimeError: If the file 'ctx_test' is not found
        """
        # Check if the file 'ctx_test' exists, and if it does, print the contents of the file, otherwise print 'None'.
        return self.console.sh(
            "if [ -f 'ctx_test' ]; then cat ctx_test; else echo 'None'; fi || true"
        )

    def get_gpu_vendor(self) -> str:
        """Get GPU vendor.

        Returns:
            str: The output of the shell command.

        Raises:
            RuntimeError: If the GPU vendor is unable to detect.

        Note:
            What types of GPU vendors are supported?
            - NVIDIA
            - AMD
        """
        # Check if the GPU vendor is NVIDIA or AMD, and if it is unable to detect the GPU vendor.
        return self.console.sh(
            'bash -c \'if [[ -f /usr/bin/nvidia-smi ]] && $(/usr/bin/nvidia-smi > /dev/null 2>&1); then echo "NVIDIA"; elif [[ -f /opt/rocm/bin/rocm-smi ]]; then echo "AMD"; elif [[ -f /usr/local/bin/rocm-smi ]]; then echo "AMD"; else echo "Unable to detect GPU vendor"; fi || true\''
        )

    def get_host_os(self) -> str:
        """Get host OS.

        Returns:
            str: The output of the shell command.

        Raises:
            RuntimeError: If the host OS is unable to detect.

        Note:
            What types of host OS are supported?
            - Ubuntu
            - CentOS
            - SLES
        """
        # Check if the host OS is Ubuntu, CentOS, SLES, or if it is unable to detect the host OS.
        return self.console.sh(
            "if [ -f \"$(which apt)\" ]; then echo 'HOST_UBUNTU'; elif [ -f \"$(which yum)\" ]; then echo 'HOST_CENTOS'; elif [ -f \"$(which zypper)\" ]; then echo 'HOST_SLES'; elif [ -f \"$(which tdnf)\" ]; then echo 'HOST_AZURE'; else echo 'Unable to detect Host OS'; fi || true"
        )

    def get_numa_balancing(self) -> bool:
        """Get NUMA balancing.

        Returns:
            bool: The output of the shell command.

        Raises:
            RuntimeError: If the NUMA balancing is not enabled or disabled.

        Note:
            NUMA balancing is enabled if the output is '1', and disabled if the output is '0'.

            What is NUMA balancing?
            Non-Uniform Memory Access (NUMA) is a computer memory design used in multiprocessing,
            where the memory access time depends on the memory location relative to the processor.
        """
        # Check if NUMA balancing is enabled or disabled.
        path = "/proc/sys/kernel/numa_balancing"
        if os.path.exists(path):
            return self.console.sh("cat /proc/sys/kernel/numa_balancing || true")
        else:
            return False

    def get_system_ngpus(self) -> int:
        """Get system number of GPUs.

        Returns:
            int: The number of GPUs.

        Raises:
            RuntimeError: If the GPU vendor is not detected.

        Note:
            What types of GPU vendors are supported?
            - NVIDIA
            - AMD
        """
        number_gpus = 0
        if self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "AMD":
            number_gpus = int(
                self.console.sh("rocm-smi --showid --csv | grep card | wc -l")
            )
        elif self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "NVIDIA":
            number_gpus = int(self.console.sh("nvidia-smi -L | wc -l"))
        else:
            raise RuntimeError("Unable to determine gpu vendor.")

        return number_gpus

    def get_system_gpu_architecture(self) -> str:
        """Get system GPU architecture.

        Returns:
            str: The GPU architecture.

        Raises:
            RuntimeError: If the GPU vendor is not detected.
            RuntimeError: If the GPU architecture is unable to determine.

        Note:
            What types of GPU vendors are supported?
            - NVIDIA
            - AMD
        """
        if self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "AMD":
            return self.console.sh("/opt/rocm/bin/rocminfo |grep -o -m 1 'gfx.*'")
        elif self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "NVIDIA":
            return self.console.sh(
                "nvidia-smi -L | head -n1 | sed 's/(UUID: .*)//g' | sed 's/GPU 0: //g'"
            )
        else:
            raise RuntimeError("Unable to determine gpu architecture.")

    def get_system_hip_version(self):
        if self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "AMD":
            return self.console.sh("hipconfig --version | cut -d'.' -f1,2")
        elif self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "NVIDIA":
            return self.console.sh(
                "nvcc --version | sed -n 's/^.*release \\([0-9]\\+\\.[0-9]\\+\\).*$/\\1/p'"
            )
        else:
            raise RuntimeError("Unable to determine hip version.")

    def get_docker_gpus(self) -> typing.Optional[str]:
        """Get Docker GPUs.

        Returns:
            str: The range of GPUs.
        """
        if int(self.ctx["docker_env_vars"]["MAD_SYSTEM_NGPUS"]) > 0:
            return "0-{}".format(
                int(self.ctx["docker_env_vars"]["MAD_SYSTEM_NGPUS"]) - 1
            )
        return None

    def get_gpu_renderD_nodes(self) -> typing.Optional[typing.List[int]]:
        """Get GPU renderD nodes from KFD properties.

        Returns:
            list: The list of GPU renderD nodes.

        Raises:
            RuntimeError: If the ROCm version is not detected

        Note:
            What is renderD?
            - renderD is the device node used for GPU rendering.
            What is KFD?
            - Kernel Fusion Driver (KFD) is the driver used for Heterogeneous System Architecture (HSA).
            What types of GPU vendors are supported?
            - AMD
        """
        # Initialize the GPU renderD nodes.
        gpu_renderDs = None
        # Check if the GPU vendor is AMD.
        if self.ctx["docker_env_vars"]["MAD_GPU_VENDOR"] == "AMD":
            # get rocm version
            rocm_version = self.console.sh(
                "cat /opt/rocm/.info/version | cut -d'-' -f1"
            )

            # get renderDs from KFD properties
            kfd_properties = self.console.sh(
                "grep -r drm_render_minor /sys/devices/virtual/kfd/kfd/topology/nodes"
            ).split("\n")
            kfd_properties = [
                line for line in kfd_properties if int(line.split()[-1]) != 0
            ]  # CPUs are 0, skip them
            kfd_renderDs = [int(line.split()[-1]) for line in kfd_properties]

            # get gpu id - renderD mapping using unique id if ROCm < 6.1.2 and node id otherwise
            # node id is more robust but is only available from 6.1.2
            if tuple(map(int, rocm_version.split("."))) < (6, 1, 2):
                kfd_unique_ids = self.console.sh(
                    "grep -r unique_id /sys/devices/virtual/kfd/kfd/topology/nodes"
                ).split("\n")
                kfd_unique_ids = [
                    hex(int(item.split()[-1])) for item in kfd_unique_ids
                ]  # get unique_id and convert it to hex

                # map unique ids to renderDs
                uniqueid_renderD_map = {
                    unique_id: renderD
                    for unique_id, renderD in zip(kfd_unique_ids, kfd_renderDs)
                }

                # get gpu id unique id map from rocm-smi
                rsmi = self.console.sh(
                    "rocm-smi --showuniqueid | grep Unique.*:"
                ).split("\n")

                # sort gpu_renderDs based on gpu ids
                gpu_renderDs = [uniqueid_renderD_map[line.split()[-1]] for line in rsmi]
            else:
                kfd_nodeids = [
                    int(re.search(r"\d+", line.split()[0]).group())
                    for line in kfd_properties
                ]

                # map node ids to renderDs
                nodeid_renderD_map = {
                    nodeid: renderD
                    for nodeid, renderD in zip(kfd_nodeids, kfd_renderDs)
                }

                # get gpu id node id map from rocm-smi
                rsmi = re.findall(r"\n\d+\s+\d+", self.console.sh("rocm-smi --showhw"))
                rsmi_gpuids = [int(s.split()[0]) for s in rsmi]
                rsmi_nodeids = [int(s.split()[1]) for s in rsmi]
                gpuid_nodeid_map = {
                    gpuid: nodeid for gpuid, nodeid in zip(rsmi_gpuids, rsmi_nodeids)
                }

                # sort gpu_renderDs based on gpu ids
                gpu_renderDs = [
                    nodeid_renderD_map[gpuid_nodeid_map[gpuid]]
                    for gpuid in sorted(gpuid_nodeid_map.keys())
                ]

        return gpu_renderDs

    def set_multi_node_runner(self) -> str:
        """
        Sets the `MAD_MULTI_NODE_RUNNER` environment variable based on the selected multi-node
        runner (e.g., `torchrun`, `mpirun`, or fallback to `python3`). This method dynamically
        generates the appropriate command based on the provided multi-node configuration.

        Returns:
            str: The command string for the multi-node runner, including necessary arguments and
            environment variable settings.
        """
        # NOTE: mpirun is untested
        if self.ctx["multi_node_args"]["RUNNER"] == "mpirun":
            if not self.ctx["multi_node_args"]["HOST_LIST"]:
                self.ctx["multi_node_args"][
                    "HOST_LIST"
                ] = f"localhost:{self.ctx['multi_node_args']['MAD_RUNTIME_NGPUS']}"
            multi_node_runner = (
                f"mpirun -np {self.ctx['multi_node_args']['NNODES'] * self.ctx['multi_node_args']['MAD_RUNTIME_NGPUS']} "
                f"--host {self.ctx['multi_node_args']['HOST_LIST']}"
            )
        else:
            distributed_args = (
                f"--nproc_per_node {self.ctx['multi_node_args']['MAD_RUNTIME_NGPUS']} "
                f"--nnodes {self.ctx['multi_node_args']['NNODES']} "
                f"--node_rank {self.ctx['multi_node_args']['NODE_RANK']} "
                f"--master_addr {self.ctx['multi_node_args']['MASTER_ADDR']} "
                f"--master_port {self.ctx['multi_node_args']['MASTER_PORT']}"
            )
            multi_node_runner = f"torchrun {distributed_args}"

        # Add NCCL and GLOO interface environment variables
        multi_node_runner = (
            f"NCCL_SOCKET_IFNAME={self.ctx['multi_node_args']['NCCL_SOCKET_IFNAME']} "
            f"GLOO_SOCKET_IFNAME={self.ctx['multi_node_args']['GLOO_SOCKET_IFNAME']} "
            f"{multi_node_runner}"
        )

        return multi_node_runner

    def _setup_build_multi_node_context(self) -> None:
        """Setup multi-node context for build phase.

        This method handles multi-node configuration during build phase,
        storing the configuration for inclusion in the manifest without requiring
        runtime GPU detection. The multi_node_args will be preserved as-is and
        MAD_MULTI_NODE_RUNNER will be generated at runtime.
        """
        if "multi_node_args" in self.ctx:
            print("Setting up multi-node context for build phase...")

            # Store the complete multi_node_args structure (excluding MAD_RUNTIME_NGPUS)
            # This will be included in build_manifest.json and used at runtime
            build_multi_node_args = {}
            for key, value in self.ctx["multi_node_args"].items():
                # Skip MAD_RUNTIME_NGPUS as it's runtime-specific - will be set at runtime
                if key != "MAD_RUNTIME_NGPUS":
                    build_multi_node_args[key] = value

            # Store the multi_node_args for inclusion in the manifest
            # This will be accessible in build_manifest.json under context
            self.ctx["build_multi_node_args"] = build_multi_node_args

            # Remove any individual MAD_MULTI_NODE_* env vars from docker_env_vars
            # Only structured multi_node_args should be stored in the manifest
            env_vars_to_remove = []
            for env_var in self.ctx.get("docker_env_vars", {}):
                if (
                    env_var.startswith("MAD_MULTI_NODE_")
                    and env_var != "MAD_MULTI_NODE_RUNNER"
                ):
                    env_vars_to_remove.append(env_var)

            for env_var in env_vars_to_remove:
                del self.ctx["docker_env_vars"][env_var]
                print(
                    f"Removed {env_var} from docker_env_vars - will be reconstructed at runtime"
                )

            print(
                f"Multi-node configuration stored for runtime: {list(build_multi_node_args.keys())}"
            )
            print("MAD_RUNTIME_NGPUS will be resolved at runtime phase")

    def _create_build_multi_node_runner_template(self) -> str:
        """Create a build-time multi-node runner command template.

        This creates a command template that uses environment variable substitution
        for runtime-specific values like MAD_RUNTIME_NGPUS.

        Returns:
            str: Command template string with environment variable placeholders
        """
        runner = self.ctx["multi_node_args"].get("RUNNER", "torchrun")

        if runner == "mpirun":
            # For mpirun, construct command with runtime substitution
            host_list = self.ctx["multi_node_args"].get("HOST_LIST", "")
            if not host_list:
                # Use runtime GPU count substitution
                multi_node_runner = (
                    "mpirun -np $(($MAD_MULTI_NODE_NNODES * ${MAD_RUNTIME_NGPUS:-1})) "
                    "--host ${MAD_MULTI_NODE_HOST_LIST:-localhost:${MAD_RUNTIME_NGPUS:-1}}"
                )
            else:
                multi_node_runner = (
                    "mpirun -np $(($MAD_MULTI_NODE_NNODES * ${MAD_RUNTIME_NGPUS:-1})) "
                    f"--host {host_list}"
                )
        else:
            # For torchrun, use environment variable substitution
            distributed_args = (
                "--nproc_per_node ${MAD_RUNTIME_NGPUS:-1} "
                "--nnodes ${MAD_MULTI_NODE_NNODES:-1} "
                "--node_rank ${MAD_MULTI_NODE_NODE_RANK:-0} "
                "--master_addr ${MAD_MULTI_NODE_MASTER_ADDR:-localhost} "
                "--master_port ${MAD_MULTI_NODE_MASTER_PORT:-6006}"
            )
            multi_node_runner = f"torchrun {distributed_args}"

        # Add NCCL and GLOO interface environment variables with conditional setting
        nccl_var = "${MAD_MULTI_NODE_NCCL_SOCKET_IFNAME:+NCCL_SOCKET_IFNAME=$MAD_MULTI_NODE_NCCL_SOCKET_IFNAME}"
        gloo_var = "${MAD_MULTI_NODE_GLOO_SOCKET_IFNAME:+GLOO_SOCKET_IFNAME=$MAD_MULTI_NODE_GLOO_SOCKET_IFNAME}"

        multi_node_runner = f"{nccl_var} {gloo_var} {multi_node_runner}"

        return multi_node_runner

    def _setup_runtime_multi_node_context(self) -> None:
        """Setup runtime multi-node context.

        This method handles multi-node configuration during runtime phase,
        setting MAD_RUNTIME_NGPUS and creating the final MAD_MULTI_NODE_RUNNER.
        """
        # Set MAD_RUNTIME_NGPUS for runtime based on detected GPU count
        if "MAD_RUNTIME_NGPUS" not in self.ctx["docker_env_vars"]:
            runtime_ngpus = self.ctx["docker_env_vars"].get("MAD_SYSTEM_NGPUS", 1)
            self.ctx["docker_env_vars"]["MAD_RUNTIME_NGPUS"] = runtime_ngpus
            print(f"Set MAD_RUNTIME_NGPUS to {runtime_ngpus} for runtime")

        # If we have multi_node_args from build phase or runtime, ensure MAD_RUNTIME_NGPUS is set
        if "multi_node_args" in self.ctx:
            # Add MAD_RUNTIME_NGPUS to multi_node_args if not already present
            if "MAD_RUNTIME_NGPUS" not in self.ctx["multi_node_args"]:
                self.ctx["multi_node_args"]["MAD_RUNTIME_NGPUS"] = self.ctx[
                    "docker_env_vars"
                ]["MAD_RUNTIME_NGPUS"]

        # If we have build_multi_node_args from manifest, reconstruct full multi_node_args
        elif "build_multi_node_args" in self.ctx:
            print("Reconstructing multi_node_args from build manifest...")
            self.ctx["multi_node_args"] = self.ctx["build_multi_node_args"].copy()
            self.ctx["multi_node_args"]["MAD_RUNTIME_NGPUS"] = self.ctx[
                "docker_env_vars"
            ]["MAD_RUNTIME_NGPUS"]

        # Generate MAD_MULTI_NODE_RUNNER if we have multi_node_args
        if "multi_node_args" in self.ctx:
            print("Creating MAD_MULTI_NODE_RUNNER with runtime values...")

            # Set individual MAD_MULTI_NODE_* environment variables for runtime execution
            # These are needed by the bash scripts that use the template runner command
            multi_node_mapping = {
                "NNODES": "MAD_MULTI_NODE_NNODES",
                "NODE_RANK": "MAD_MULTI_NODE_NODE_RANK",
                "MASTER_ADDR": "MAD_MULTI_NODE_MASTER_ADDR",
                "MASTER_PORT": "MAD_MULTI_NODE_MASTER_PORT",
                "NCCL_SOCKET_IFNAME": "MAD_MULTI_NODE_NCCL_SOCKET_IFNAME",
                "GLOO_SOCKET_IFNAME": "MAD_MULTI_NODE_GLOO_SOCKET_IFNAME",
                "HOST_LIST": "MAD_MULTI_NODE_HOST_LIST",
            }

            for multi_node_key, env_var_name in multi_node_mapping.items():
                if multi_node_key in self.ctx["multi_node_args"]:
                    self.ctx["docker_env_vars"][env_var_name] = str(
                        self.ctx["multi_node_args"][multi_node_key]
                    )
                    print(
                        f"Set {env_var_name} to {self.ctx['multi_node_args'][multi_node_key]} for runtime"
                    )

            # Generate the MAD_MULTI_NODE_RUNNER command
            self.ctx["docker_env_vars"][
                "MAD_MULTI_NODE_RUNNER"
            ] = self.set_multi_node_runner()
            print(
                f"MAD_MULTI_NODE_RUNNER: {self.ctx['docker_env_vars']['MAD_MULTI_NODE_RUNNER']}"
            )

    def filter(self, unfiltered: typing.Dict) -> typing.Dict:
        """Filter the unfiltered dictionary based on the context.

        Args:
            unfiltered: The unfiltered dictionary.

        Returns:
            dict: The filtered dictionary.
        """
        # Initialize the filtered dictionary.
        filtered = {}
        # Iterate over the unfiltered dictionary and filter based on the context
        for dockerfile in unfiltered.keys():
            # Convert the string representation of python dictionary to a dictionary.
            dockerctx = ast.literal_eval(unfiltered[dockerfile])
            # logic : if key is in the Dockerfile, it has to match current context
            # if context is empty in Dockerfile, it will match
            match = True
            # Iterate over the docker context and check if the context matches the current context.
            for dockerctx_key in dockerctx.keys():
                if (
                    dockerctx_key in self.ctx
                    and dockerctx[dockerctx_key] != self.ctx[dockerctx_key]
                ):
                    match = False
                    continue
            # If the context matches, add it to the filtered dictionary.
            if match:
                filtered[dockerfile] = unfiltered[dockerfile]
        return filtered
